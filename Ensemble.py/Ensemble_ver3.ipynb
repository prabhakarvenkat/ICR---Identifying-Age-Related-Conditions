{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443dea59",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-22T15:28:52.989938Z",
     "iopub.status.busy": "2023-07-22T15:28:52.989558Z",
     "iopub.status.idle": "2023-07-22T15:28:53.004941Z",
     "shell.execute_reply": "2023-07-22T15:28:53.003475Z"
    },
    "papermill": {
     "duration": 0.022236,
     "end_time": "2023-07-22T15:28:53.007513",
     "exception": false,
     "start_time": "2023-07-22T15:28:52.985277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv\n",
      "/kaggle/input/icr-identify-age-related-conditions/greeks.csv\n",
      "/kaggle/input/icr-identify-age-related-conditions/train.csv\n",
      "/kaggle/input/icr-identify-age-related-conditions/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06888dc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T15:28:53.014051Z",
     "iopub.status.busy": "2023-07-22T15:28:53.013675Z",
     "iopub.status.idle": "2023-07-22T15:29:55.405729Z",
     "shell.execute_reply": "2023-07-22T15:29:55.404472Z"
    },
    "papermill": {
     "duration": 62.401865,
     "end_time": "2023-07-22T15:29:55.411881",
     "exception": false,
     "start_time": "2023-07-22T15:28:53.010016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6923979\ttest: 0.6924427\tbest: 0.6924427 (0)\ttotal: 108ms\tremaining: 1m 48s\n",
      "50:\tlearn: 0.6582626\ttest: 0.6587053\tbest: 0.6587053 (50)\ttotal: 2.19s\tremaining: 40.8s\n",
      "100:\tlearn: 0.6257883\ttest: 0.6265728\tbest: 0.6265728 (100)\ttotal: 4.32s\tremaining: 38.5s\n",
      "150:\tlearn: 0.5956427\ttest: 0.5969268\tbest: 0.5969268 (150)\ttotal: 6.4s\tremaining: 36s\n",
      "200:\tlearn: 0.5670357\ttest: 0.5687569\tbest: 0.5687569 (200)\ttotal: 8.48s\tremaining: 33.7s\n",
      "250:\tlearn: 0.5400053\ttest: 0.5421171\tbest: 0.5421171 (250)\ttotal: 10.6s\tremaining: 31.5s\n",
      "300:\tlearn: 0.5141058\ttest: 0.5163251\tbest: 0.5163251 (300)\ttotal: 12.6s\tremaining: 29.4s\n",
      "350:\tlearn: 0.4902612\ttest: 0.4928024\tbest: 0.4928024 (350)\ttotal: 14.9s\tremaining: 27.5s\n",
      "400:\tlearn: 0.4678021\ttest: 0.4704835\tbest: 0.4704835 (400)\ttotal: 16.9s\tremaining: 25.3s\n",
      "450:\tlearn: 0.4463697\ttest: 0.4492138\tbest: 0.4492138 (450)\ttotal: 19s\tremaining: 23.2s\n",
      "500:\tlearn: 0.4260241\ttest: 0.4289115\tbest: 0.4289115 (500)\ttotal: 21.1s\tremaining: 21s\n",
      "550:\tlearn: 0.4070668\ttest: 0.4102113\tbest: 0.4102113 (550)\ttotal: 23.2s\tremaining: 18.9s\n",
      "600:\tlearn: 0.3890535\ttest: 0.3919810\tbest: 0.3919810 (600)\ttotal: 25.3s\tremaining: 16.8s\n",
      "650:\tlearn: 0.3720387\ttest: 0.3751585\tbest: 0.3751585 (650)\ttotal: 27.4s\tremaining: 14.7s\n",
      "700:\tlearn: 0.3557979\ttest: 0.3587282\tbest: 0.3587282 (700)\ttotal: 29.5s\tremaining: 12.6s\n",
      "750:\tlearn: 0.3408435\ttest: 0.3438792\tbest: 0.3438792 (750)\ttotal: 32s\tremaining: 10.6s\n",
      "800:\tlearn: 0.3266990\ttest: 0.3297900\tbest: 0.3297900 (800)\ttotal: 34.1s\tremaining: 8.47s\n",
      "850:\tlearn: 0.3132256\ttest: 0.3161053\tbest: 0.3161053 (850)\ttotal: 36.2s\tremaining: 6.34s\n",
      "900:\tlearn: 0.3007737\ttest: 0.3035895\tbest: 0.3035895 (900)\ttotal: 38.3s\tremaining: 4.21s\n",
      "950:\tlearn: 0.2888735\ttest: 0.2919739\tbest: 0.2919739 (950)\ttotal: 40.4s\tremaining: 2.08s\n",
      "999:\tlearn: 0.2777230\ttest: 0.2808725\tbest: 0.2808725 (999)\ttotal: 42.4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.2808725286\n",
      "bestIteration = 999\n",
      "\n",
      "Validation Balanced Log Loss (Ensemble): 0.018252140631640396\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred, sample_weight=None):\n",
    "    N_0 = np.sum(1 - y_true)\n",
    "    N_1 = np.sum(y_true)\n",
    "    w_0 = 1 / N_0\n",
    "    w_1 = 1 / N_1\n",
    "    p_1 = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    p_0 = 1 - p_1\n",
    "    log_loss_0 = -np.sum((1 - y_true) * np.log(p_0))\n",
    "    log_loss_1 = -np.sum(y_true * np.log(p_1))\n",
    "    balanced_log_loss = 2 * (w_0 * log_loss_0 + w_1 * log_loss_1) / (w_0 + w_1)\n",
    "    return balanced_log_loss / (N_0 + N_1)\n",
    "\n",
    "# Read the data\n",
    "train_df = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "train_df[\"EJ\"] = train_df[\"EJ\"].replace({\"A\": 0, \"B\": 1})\n",
    "train_df.fillna(train_df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "test_df[\"EJ\"] = test_df[\"EJ\"].replace({\"A\": 0, \"B\": 1})\n",
    "test_df.fillna(test_df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "# Apply KNN imputation to handle missing values in training data\n",
    "knn_imputer = KNNImputer(n_neighbors=20)\n",
    "train_df_imputed = knn_imputer.fit_transform(train_df.drop([\"Class\", \"Id\"], axis=1))\n",
    "train_df_imputed = pd.DataFrame(train_df_imputed, columns=train_df.drop([\"Class\", \"Id\"], axis=1).columns)\n",
    "\n",
    "# Apply KNN imputation to handle missing values in test data\n",
    "test_df_imputed = knn_imputer.transform(test_df.drop(\"Id\", axis=1))\n",
    "test_df_imputed = pd.DataFrame(test_df_imputed, columns=test_df.drop(\"Id\", axis=1).columns)\n",
    "\n",
    "# Feature selection: Select k-best features\n",
    "k_best_features = 30  # Choose the number of best features you want to select\n",
    "selector = SelectKBest(f_classif, k=k_best_features)\n",
    "X_train_kbest = selector.fit_transform(train_df_imputed, train_df[\"Class\"])\n",
    "X_test_kbest = selector.transform(test_df_imputed)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_kbest, train_df[\"Class\"],\n",
    "                                                  test_size=0.1, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training set to oversample the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train_kbest, train_df[\"Class\"])\n",
    "\n",
    "# Apply Random Under-sampling to balance the class distribution\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_train, y_train = under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Data preprocessing: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test_kbest_scaled = scaler.transform(X_test_kbest)\n",
    "\n",
    "# CatBoost Model with extended early stopping and L1/L2 regularization\n",
    "num_positive_samples = len(y_train[y_train == 1])\n",
    "num_negative_samples = len(y_train[y_train == 0])\n",
    "scale_pos_weight = num_negative_samples / num_positive_samples\n",
    "catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.0005, depth=10, reg_lambda=1,\n",
    "                                    random_strength=1, loss_function='Logloss',\n",
    "                                    scale_pos_weight=scale_pos_weight, verbose=False)\n",
    "catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=20, verbose_eval=50)\n",
    "\n",
    "# LightGBM Model with L1 and L2 regularization\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'lambda_l1': 0.1,  # L1 regularization term\n",
    "    'lambda_l2': 0.1,  # L2 regularization term\n",
    "    'verbose': -1\n",
    "}\n",
    "lightgbm_model = lgb.train(lgb_params, lgb_train, num_boost_round=2000)\n",
    "\n",
    "# XGBoost Model with L1 and L2 regularization\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.01,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'lambda': 0.1,  # L1 regularization term \n",
    "    'alpha': 0.1,   # L2 regularization term \n",
    "    'seed': 42\n",
    "}\n",
    "xgb_train = xgb.DMatrix(X_train, label = y_train)\n",
    "xgb_model = xgb.train(xgb_params, xgb_train, num_boost_round = 2000)\n",
    "\n",
    "# Predictions on the validation set\n",
    "val_preds_catboost = catboost_model.predict_proba(X_val)[ :, 1]\n",
    "val_preds_lightgbm = lightgbm_model.predict(X_val)\n",
    "val_preds_xgboost = xgb_model.predict(xgb.DMatrix(X_val))\n",
    "\n",
    "# Weights for weighted average ensemble\n",
    "weight_catboost = 0.1\n",
    "weight_lightgbm = 0.8\n",
    "weight_xgboost = 0.1\n",
    "\n",
    "# Weighted ensemble predictions on the validation set\n",
    "val_preds_ensemble_weighted = (weight_catboost * val_preds_catboost +\n",
    "                              weight_lightgbm * val_preds_lightgbm +\n",
    "                              weight_xgboost * val_preds_xgboost)\n",
    "\n",
    "# Evaluate the ensemble model with balanced_log_loss on the validation set\n",
    "ensemble_loss_weighted = balanced_log_loss(y_val, val_preds_ensemble_weighted)\n",
    "print(f\"Validation Balanced Log Loss (Ensemble): {ensemble_loss_weighted}\")\n",
    "\n",
    "# Predictions on the test data for each model\n",
    "test_preds_xgboost = xgb_model.predict(xgb.DMatrix(X_test_kbest_scaled))\n",
    "test_preds_catboost = catboost_model.predict_proba(X_test_kbest_scaled)[ :, 1]\n",
    "test_preds_lightgbm = lightgbm_model.predict(X_test_kbest_scaled)\n",
    "\n",
    "# Weighted ensemble predictions on the test data\n",
    "test_preds_ensemble_weighted = (weight_catboost * test_preds_catboost +\n",
    "                                weight_lightgbm * test_preds_lightgbm +\n",
    "                                weight_xgboost * test_preds_xgboost)\n",
    "\n",
    "# Prepare the submission using the ensemble model predictions\n",
    "sample_submission_df = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "sample_submission_df['Id'] = test_df.reset_index()['Id']\n",
    "sample_submission_df[\"class_0\"] = 1 - test_preds_ensemble_weighted\n",
    "sample_submission_df[\"class_1\"] = test_preds_ensemble_weighted\n",
    "sample_submission_df.set_index('Id').to_csv('submission.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 75.368244,
   "end_time": "2023-07-22T15:29:56.640334",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-22T15:28:41.272090",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
